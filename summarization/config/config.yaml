data:
    finetuning_dataset: /opt/ml/input/summarization_data/
    overwrite_cache: False
    output_dir: ./models/test
    resume_from_checkpoint: None
    cache_dir: 

model:
    model_name_or_path: gogamza/kobart-base-v2
    # model_name_or_path : yeombora/dialogue_summarization
    roberta_path : klue/roberta-base
    use_fast: True

arg:
    ignore_pad_token_for_loss: True
    pad_to_max_length: True
    max_source_length: 512
    max_target_length: 512
    source_prefix: None
    preprocessing_num_workers: # None    
    max_train_samples: # None
    val_max_target_length: # None
    max_eval_samples: # None
    num_beams: # None
    resize_position_embeddings: # None
    lr : 5e-5 
    batch_size : 32
    num_epochs : 5 
    save_dir : "/opt/ml/final-project-level3-nlp-07/summarization/r3f_model"
    weight_decay : 0.1
    margin_lambda : 0.01
    eval_steps : 1500
    early_stopping_patience : -1
    
train:
    seed: 42
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 32
    gradient_accumulation_steps: 2
    learning_rate: 5e-5
    num_train_epochs: 3
    weight_decay: 0.01
    label_smoothing_factor: 0.01
    logging_steps: 500
    eval_steps: 4000
    save_steps: 4000
    overwrite_output_dir: False

huggingface:
    push_to_hub: False
    hub_private_repo: True
    hub_token: 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       # 염보라 token

wandb:
    wandb_mode: False
    entity: final-bora
    projcet_name: test
    exp_name: test